#!/usr/bin/env python3
# =============================================================
# codeGuardian LoRA Fine-tuning Pipeline v3.1 (Optimized)
# Model: microsoft/graphcodebert-base
# Author: Urva Gandhi
# Version: v3.1 â€“ Stability & Speed Optimized
# =============================================================
"""
Production-grade LoRA fine-tuning for GraphCodeBERT vulnerability detection.
Optimized for Kaggle Free GPU (T4/P100) with enhanced stability and 10-20% speedup.

Features:
- âœ… Automatic checkpoint resume (restartable training)
- âœ… Safe torch.compile with OOM fallback
- âœ… NaN-safe backward pass with FP16 guards
- âœ… Proper LoRA merge with base model
- âœ… HuggingFace Hub upload via CLI
- âœ… Full state persistence (optimizer, scheduler, epoch, best F1)
- âœ… Mixed precision (BF16/FP16 auto-detect)
- âœ… Weighted cross-entropy + optional Focal Loss
- âœ… Early stopping on F1 score
- âœ… Gradient accumulation & checkpointing
- âœ… Enhanced LoRA (r=16, Î±=32, query+key+value)
- âœ… Production-grade error handling
"""

import os
import sys
import gc
import json
import time
import logging
import argparse
import hashlib
import subprocess
import warnings
from typing import Dict, Tuple, Optional
from pathlib import Path
from datetime import datetime

import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader, TensorDataset
from torch import amp
from transformers import AutoModel, AutoTokenizer, get_linear_schedule_with_warmup
from peft import LoraConfig, get_peft_model, PeftModel, PeftConfig
from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix
from tqdm import tqdm

# Reproducibility
SEED = 42
torch.manual_seed(SEED)
np.random.seed(SEED)
if torch.cuda.is_available():
    torch.cuda.manual_seed_all(SEED)
    torch.backends.cudnn.deterministic = True
    # Enable TF32 for Ampere+ GPUs
    try:
        torch.backends.cuda.matmul.allow_tf32 = True
        torch.backends.cudnn.allow_tf32 = True
    except Exception:
        pass
    # Keep cudnn.benchmark for GraphCodeBERT (benefits from kernel caching)
    torch.backends.cudnn.benchmark = True

# Suppress torch.compile warnings
warnings.filterwarnings("ignore", message=".*Not enough SMs to use max_autotune_gemm.*")

# ============================================================================
# GPU DETECTION
# ============================================================================

def check_bf16_support() -> bool:
    """Check if GPU supports BFloat16 (compute capability >= 8.0)"""
    if not torch.cuda.is_available():
        return False
    capability = torch.cuda.get_device_capability()
    return capability[0] >= 8

BF16_SUPPORTED = check_bf16_support()

# ============================================================================
# CONFIGURATION
# ============================================================================

class Config:
    """Training configuration"""

    # Model
    MODEL_NAME = "microsoft/graphcodebert-base"
    NUM_LABELS = 2

    # LoRA (Enhanced for GraphCodeBERT)
    LORA_R = 16
    LORA_ALPHA = 32
    LORA_DROPOUT = 0.1
    LORA_TARGET_MODULES = ["query", "key", "value"]

    # Training
    EPOCHS = 3
    TRAIN_BATCH_SIZE = 8  # Will be autotuned if needed
    EVAL_BATCH_SIZE = 64  # Larger eval batch (pure inference, no gradients)
    LEARNING_RATE = 3e-5
    WEIGHT_DECAY = 1e-2
    MAX_GRAD_NORM = 1.0
    WARMUP_RATIO = 0.05
    GRADIENT_ACCUMULATION_STEPS = 2

    # Early stopping
    EARLY_STOPPING_PATIENCE = 2
    EARLY_STOPPING_MIN_DELTA = 0.001

    # Loss
    USE_WEIGHTED_LOSS = True
    USE_FOCAL_LOSS = False
    FOCAL_GAMMA = 2.0

    # Paths
    if os.path.exists("/kaggle/input"):
        # Use local SSD instead of NFS for 2x I/O speedup
        DATA_PATH = "/kaggle/temp_data/codeguardian-dataset-for-model-fine-tuning/tokenized/graphcodebert-base"
        OUTPUT_DIR = "/kaggle/working"
    else:
        DATA_PATH = "datasets/tokenized/graphcodebert"
        OUTPUT_DIR = "outputs"

    CHECKPOINT_DIR = f"{OUTPUT_DIR}/checkpoints_graphcodebert"
    METRICS_DIR = f"{OUTPUT_DIR}/metrics_graphcodebert"
    LOG_DIR = f"{OUTPUT_DIR}/logs"
    FINAL_MODEL_DIR = f"{OUTPUT_DIR}/graphcodebert_lora_final"
    MERGED_MODEL_DIR = f"{OUTPUT_DIR}/graphcodebert_lora_merged"

    # HuggingFace
    HF_REPO_ID = "urva-gandhi/codeGuardian-GraphCodeBERT"

    # Device
    DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    USE_MIXED_PRECISION = True
    PRECISION_DTYPE = torch.bfloat16 if BF16_SUPPORTED else torch.float16
    GRADIENT_CHECKPOINTING = False
    NUM_WORKERS = 4  # Optimal for persistent workers with TensorDataset

# ============================================================================
# LOGGING
# ============================================================================

def generate_run_hash(config: 'Config') -> str:
    """Generate reproducibility hash from training configuration"""
    config_str = f"{config.MODEL_NAME}_{config.LORA_R}_{config.LORA_ALPHA}_" \
                 f"{config.LEARNING_RATE}_{config.TRAIN_BATCH_SIZE}_{config.EPOCHS}_" \
                 f"{SEED}_{config.USE_WEIGHTED_LOSS}_{config.USE_FOCAL_LOSS}"
    return hashlib.md5(config_str.encode()).hexdigest()[:8]

def setup_logging(log_dir: str) -> logging.Logger:
    """Setup logging to file and console"""
    os.makedirs(log_dir, exist_ok=True)

    logger = logging.getLogger("GraphCodeBERT_LoRA_v3_1")
    logger.setLevel(logging.INFO)
    logger.handlers.clear()

    formatter = logging.Formatter(
        "%(asctime)s - %(levelname)s - %(message)s",
        datefmt="%Y-%m-%d %H:%M:%S"
    )

    fh = logging.FileHandler(os.path.join(log_dir, "train_log.txt"), mode="a")
    fh.setFormatter(formatter)
    logger.addHandler(fh)

    ch = logging.StreamHandler()
    ch.setFormatter(formatter)
    logger.addHandler(ch)

    return logger

# ============================================================================
# LOSS FUNCTIONS
# ============================================================================

class FocalLoss(nn.Module):
    """Focal Loss for handling class imbalance"""

    def __init__(self, gamma: float = 2.0, alpha: Optional[torch.Tensor] = None):
        super().__init__()
        self.gamma = gamma
        self.alpha = alpha

    def forward(self, inputs: torch.Tensor, targets: torch.Tensor) -> torch.Tensor:
        bce_loss = nn.functional.cross_entropy(inputs, targets, reduction="none")
        pt = torch.exp(-bce_loss)
        focal_loss = ((1 - pt) ** self.gamma) * bce_loss
        if self.alpha is not None:
            focal_loss = self.alpha[targets] * focal_loss
        return focal_loss.mean()

# ============================================================================
# DATA LOADING
# ============================================================================

def load_tokenized_dataset(file_path: str, logger: logging.Logger) -> TensorDataset:
    """Load pre-tokenized dataset from .pt file"""
    logger.info(f"Loading: {file_path}")

    if not os.path.exists(file_path):
        raise FileNotFoundError(f"Dataset not found: {file_path}")

    data = torch.load(file_path, map_location="cpu")

    # Extract tensors
    input_ids = data["input_ids"]
    attention_mask = data["attention_mask"]
    labels = data["labels"]

    logger.info(f"  Samples: {len(labels):,}")
    logger.info(f"  Shape: {input_ids.shape}")
    logger.info(f"  Labels: {torch.bincount(labels).tolist()}")

    return TensorDataset(input_ids, attention_mask, labels)

def create_dataloaders(config: Config, logger: logging.Logger) -> Tuple[DataLoader, DataLoader, DataLoader]:
    """Create train, validation, and test dataloaders"""
    logger.info("=" * 70)
    logger.info("LOADING DATASETS")
    logger.info("=" * 70)

    train_dataset = load_tokenized_dataset(
        os.path.join(config.DATA_PATH, "train_tokenized_graphcodebert-base.pt"), logger
    )
    val_dataset = load_tokenized_dataset(
        os.path.join(config.DATA_PATH, "val_tokenized_graphcodebert-base.pt"), logger
    )
    test_dataset = load_tokenized_dataset(
        os.path.join(config.DATA_PATH, "test_tokenized_graphcodebert-base.pt"), logger
    )

    train_loader = DataLoader(
        train_dataset,
        batch_size=config.TRAIN_BATCH_SIZE,
        shuffle=True,
        num_workers=config.NUM_WORKERS,
        pin_memory=True,
        persistent_workers=True,
        prefetch_factor=8
    )
    val_loader = DataLoader(
        val_dataset,
        batch_size=config.EVAL_BATCH_SIZE,
        shuffle=False,
        num_workers=config.NUM_WORKERS,
        pin_memory=True,
        persistent_workers=True,
        prefetch_factor=8
    )
    test_loader = DataLoader(
        test_dataset,
        batch_size=config.EVAL_BATCH_SIZE,
        shuffle=False,
        num_workers=config.NUM_WORKERS,
        pin_memory=True,
        persistent_workers=True,
        prefetch_factor=8
    )

    logger.info(f"Train batches: {len(train_loader)}")
    logger.info(f"Val batches: {len(val_loader)}")
    logger.info(f"Test batches: {len(test_loader)}")
    logger.info(f"Actual train micro-batch size: {train_loader.batch_size}")
    logger.info(f"Actual eval micro-batch size: {val_loader.batch_size}")
    logger.info(f"Iterations per epoch: {len(train_loader):,}")

    return train_loader, val_loader, test_loader

# ============================================================================
# MODEL
# ============================================================================

class GraphCodeBERTClassifier(nn.Module):
    """GraphCodeBERT with classification head"""

    def __init__(self, model_name: str, num_labels: int = 2):
        super().__init__()
        self.roberta = AutoModel.from_pretrained(model_name)
        self.config = self.roberta.config  # Expose config for PEFT compatibility
        self.classifier = nn.Sequential(
            nn.Dropout(0.1),
            nn.Linear(self.roberta.config.hidden_size, num_labels)
        )

        # Freeze backbone
        for param in self.roberta.parameters():
            param.requires_grad = False

    def forward(self, input_ids, attention_mask, **kwargs):
        outputs = self.roberta(input_ids=input_ids, attention_mask=attention_mask)
        # Use mean pooling instead of pooler_output (avoids 768x768 matmul)
        pooled = torch.mean(outputs.last_hidden_state, dim=1)
        logits = self.classifier(pooled)
        return logits

def compute_class_weights(labels: torch.Tensor, device: torch.device) -> torch.Tensor:
    """Compute class weights for imbalanced datasets"""
    class_counts = torch.bincount(labels)
    total = len(labels)
    weights = total / (len(class_counts) * class_counts.float())
    # Add epsilon to prevent division by zero
    weights = torch.clamp(weights, min=1e-6)
    return weights.to(device)

def validate_lora_targets(model, target_modules: list, logger: logging.Logger) -> bool:
    """Validate that LoRA target modules exist in model"""
    model_modules = set()
    for name, _ in model.named_modules():
        if "." in name:
            model_modules.add(name.split(".")[-1])
    
    missing = [t for t in target_modules if t not in model_modules]
    if missing:
        logger.warning(f"âš ï¸ LoRA targets not found in model: {missing}")
        logger.info(f"Available modules: {sorted(model_modules)}")
        return False
    return True

def initialize_model(config: Config, logger: logging.Logger) -> nn.Module:
    """Initialize model with LoRA"""
    logger.info("=" * 70)
    logger.info("INITIALIZING MODEL")
    logger.info("=" * 70)

    model = GraphCodeBERTClassifier(config.MODEL_NAME, config.NUM_LABELS)
    logger.info(f"Base model: {config.MODEL_NAME}")

    total_params = sum(p.numel() for p in model.parameters())
    trainable_before = sum(p.numel() for p in model.parameters() if p.requires_grad)

    logger.info(f"Total params: {total_params:,}")
    logger.info(f"Trainable before LoRA: {trainable_before:,} ({100*trainable_before/total_params:.2f}%)")

    # Validate LoRA targets
    validate_lora_targets(model, config.LORA_TARGET_MODULES, logger)

    # Apply LoRA
    lora_config = LoraConfig(
        task_type="SEQ_CLS",
        r=config.LORA_R,
        lora_alpha=config.LORA_ALPHA,
        lora_dropout=config.LORA_DROPOUT,
        target_modules=config.LORA_TARGET_MODULES,
        bias="none",
        inference_mode=False
    )

    model = get_peft_model(model, lora_config)

    if config.GRADIENT_CHECKPOINTING:
        try:
            if hasattr(model.base_model, "roberta"):
                model.base_model.roberta.gradient_checkpointing_enable()
            elif hasattr(model.base_model, "model") and hasattr(model.base_model.model, "roberta"):
                model.base_model.model.roberta.gradient_checkpointing_enable()
            logger.info("âœ“ Gradient checkpointing enabled")
        except Exception as e:
            logger.warning(f"Could not enable gradient checkpointing: {e}")

    trainable_after = sum(p.numel() for p in model.parameters() if p.requires_grad)

    logger.info(f"LoRA config: r={config.LORA_R}, Î±={config.LORA_ALPHA}, dropout={config.LORA_DROPOUT}")
    logger.info(f"Target modules: {config.LORA_TARGET_MODULES}")
    logger.info(f"Trainable after LoRA: {trainable_after:,} ({100*trainable_after/total_params:.2f}%)")
    logger.info(f"Memory reduction: {100*(1-trainable_after/total_params):.1f}%")

    return model.to(config.DEVICE)

# ============================================================================
# CHECKPOINT MANAGEMENT
# ============================================================================

def save_checkpoint(
    epoch: int, model, optimizer, scheduler, best_f1: float,
    checkpoint_dir: str, logger: logging.Logger, is_compiled: bool = False
) -> None:
    """Save training checkpoint (handles compiled models)"""
    os.makedirs(checkpoint_dir, exist_ok=True)

    checkpoint_path = os.path.join(checkpoint_dir, f"checkpoint_epoch_{epoch}.pt")

    checkpoint = {
        "epoch": epoch,
        "best_f1": best_f1,
        "optimizer_state": optimizer.state_dict(),
        "scheduler_state": scheduler.state_dict(),
        "is_compiled": is_compiled
    }

    # Extract base model if compiled
    save_model = model._orig_mod if is_compiled and hasattr(model, "_orig_mod") else model
    
    # Save PEFT model separately
    save_model.save_pretrained(checkpoint_dir)

    # Save training state
    torch.save(checkpoint, checkpoint_path)
    logger.info(f"âœ“ Checkpoint saved: epoch {epoch}, F1={best_f1:.4f}")

def load_checkpoint(
    checkpoint_dir: str, model, optimizer, scheduler, config: Config, logger: logging.Logger
) -> Tuple[int, float, bool]:
    """Load training checkpoint if exists"""

    if not os.path.exists(checkpoint_dir):
        logger.info("No checkpoint found - starting from scratch")
        return 0, 0.0, False

    checkpoints = [f for f in os.listdir(checkpoint_dir) if f.startswith("checkpoint_epoch_")]
    if not checkpoints:
        logger.info("No checkpoint found - starting from scratch")
        return 0, 0.0, False

    epochs = [int(f.split("_")[-1].replace(".pt", "")) for f in checkpoints]
    latest_epoch = max(epochs)
    checkpoint_path = os.path.join(checkpoint_dir, f"checkpoint_epoch_{latest_epoch}.pt")

    try:
        # Load training state first
        checkpoint = torch.load(checkpoint_path, map_location=config.DEVICE)
        
        # Safe extraction of base model (handles compiled models)
        model_uncompiled = getattr(model, "_orig_mod", model)
        base_model = model_uncompiled.base_model.model if hasattr(model_uncompiled, "base_model") and hasattr(model_uncompiled.base_model, "model") else model_uncompiled
        
        # Load PEFT model
        model = PeftModel.from_pretrained(base_model, checkpoint_dir).to(config.DEVICE)

        optimizer.load_state_dict(checkpoint["optimizer_state"])
        scheduler.load_state_dict(checkpoint["scheduler_state"])
        best_f1 = checkpoint["best_f1"]
        is_compiled = checkpoint.get("is_compiled", False)

        logger.info("=" * 70)
        logger.info("ðŸ” RESUMING FROM CHECKPOINT")
        logger.info("=" * 70)
        logger.info(f"âœ“ Loaded checkpoint: epoch {latest_epoch}, best F1={best_f1:.4f}")
        logger.info(f"ðŸ”„ Resumed from epoch {latest_epoch} (best F1={best_f1:.4f})")

        return latest_epoch, best_f1, is_compiled

    except Exception as e:
        logger.warning(f"Failed to load checkpoint: {e}")
        logger.info("Starting from scratch")
        return 0, 0.0, False

# ============================================================================
# TRAINING
# ============================================================================

def calculate_metrics(predictions: np.ndarray, labels: np.ndarray) -> Dict[str, float]:
    """Calculate evaluation metrics"""
    return {
        "accuracy": accuracy_score(labels, predictions),
        "f1": f1_score(labels, predictions, average="binary", zero_division=0),
        "precision": precision_score(labels, predictions, average="binary", zero_division=0),
        "recall": recall_score(labels, predictions, average="binary", zero_division=0)
    }

def train_epoch(
    model, train_loader, optimizer, scheduler, scaler, criterion,
    config: Config, logger: logging.Logger, epoch: int
) -> Dict[str, float]:
    """Train for one epoch with NaN safety"""
    model.train()
    total_loss = 0
    all_preds = []
    all_labels = []
    nan_skips = 0

    progress_bar = tqdm(train_loader, desc=f"Epoch {epoch}/{config.EPOCHS} [TRAIN]")
    optimizer.zero_grad()

    for batch_idx, batch in enumerate(progress_bar):
        input_ids = batch[0].to(config.DEVICE, non_blocking=True)
        attention_mask = batch[1].to(config.DEVICE, non_blocking=True)
        labels = batch[2].to(config.DEVICE, non_blocking=True)

        if config.USE_MIXED_PRECISION:
            with amp.autocast("cuda", dtype=config.PRECISION_DTYPE):
                logits = model(input_ids=input_ids, attention_mask=attention_mask)
                loss = criterion(logits, labels) / config.GRADIENT_ACCUMULATION_STEPS

            # NaN safety check
            if torch.isnan(loss):
                logger.warning(f"âš ï¸ NaN loss detected at step {batch_idx+1}, skipping")
                nan_skips += 1
                optimizer.zero_grad()
                continue

            scaler.scale(loss).backward()

            if (batch_idx + 1) % config.GRADIENT_ACCUMULATION_STEPS == 0:
                scaler.unscale_(optimizer)
                torch.nn.utils.clip_grad_norm_(model.parameters(), config.MAX_GRAD_NORM)
                scaler.step(optimizer)
                scaler.update()
                scheduler.step()
                optimizer.zero_grad()
        else:
            logits = model(input_ids=input_ids, attention_mask=attention_mask)
            loss = criterion(logits, labels) / config.GRADIENT_ACCUMULATION_STEPS
            
            if torch.isnan(loss):
                logger.warning(f"âš ï¸ NaN loss detected at step {batch_idx+1}, skipping")
                nan_skips += 1
                optimizer.zero_grad()
                continue
                
            loss.backward()

            if (batch_idx + 1) % config.GRADIENT_ACCUMULATION_STEPS == 0:
                torch.nn.utils.clip_grad_norm_(model.parameters(), config.MAX_GRAD_NORM)
                optimizer.step()
                scheduler.step()
                optimizer.zero_grad()

        total_loss += loss.item() * config.GRADIENT_ACCUMULATION_STEPS
        preds = torch.argmax(logits, dim=1).cpu().numpy()
        all_preds.extend(preds)
        all_labels.extend(labels.cpu().numpy())

        progress_bar.set_postfix({"loss": f"{total_loss/(batch_idx+1):.4f}"})

        # Log every 5000 steps (use NumPy for speed)
        if (batch_idx + 1) % 5000 == 0:
            step_loss = total_loss / (batch_idx + 1)
            tail_p = np.array(all_preds[-5000:], dtype=np.int32)
            tail_l = np.array(all_labels[-5000:], dtype=np.int32)
            acc = float((tail_p == tail_l).mean())
            logger.info(f"  [Epoch {epoch}] Step {batch_idx+1}/{len(train_loader)} | Loss={step_loss:.4f} | Acc={acc:.4f}")

    if nan_skips > 0:
        logger.warning(f"âš ï¸ Skipped {nan_skips} NaN losses this epoch")

    # Explicit cleanup
    torch.cuda.empty_cache()
    gc.collect()

    metrics = calculate_metrics(np.array(all_preds), np.array(all_labels))
    metrics["loss"] = total_loss / len(train_loader)
    return metrics

def evaluate(
    model, data_loader, criterion, config: Config, logger: logging.Logger, split_name: str
) -> Dict[str, float]:
    """Evaluate model"""
    model.eval()
    total_loss = 0
    all_preds = []
    all_labels = []
    
    with torch.no_grad():
        for batch in tqdm(data_loader, desc=f"{split_name.upper()}"):
            input_ids = batch[0].to(config.DEVICE, non_blocking=True)
            attention_mask = batch[1].to(config.DEVICE, non_blocking=True)
            # âš¡ BUG FIX: Removed duplicate .DEVICE typo
            labels = batch[2].to(config.DEVICE, non_blocking=True)

            if config.USE_MIXED_PRECISION:
                with amp.autocast("cuda", dtype=config.PRECISION_DTYPE):
                    logits = model(input_ids=input_ids, attention_mask=attention_mask)
                    loss = criterion(logits, labels)
            else:
                logits = model(input_ids=input_ids, attention_mask=attention_mask)
                loss = criterion(logits, labels)

            total_loss += loss.item()
            preds = torch.argmax(logits, dim=1).cpu().numpy()
            all_preds.extend(preds)
            all_labels.extend(labels.cpu().numpy())

    metrics = calculate_metrics(np.array(all_preds), np.array(all_labels))
    metrics["loss"] = total_loss / len(data_loader)
    metrics["predictions"] = np.array(all_preds)
    metrics["labels"] = np.array(all_labels)
    return metrics

# ============================================================================
# MODEL MERGING & UPLOAD
# ============================================================================

def merge_lora_model(config: Config, logger: logging.Logger) -> bool:
    """Merge LoRA adapters with base model"""
    logger.info("\n" + "=" * 70)
    logger.info("MERGING LORA ADAPTERS")
    logger.info("=" * 70)

    try:
        peft_config = PeftConfig.from_pretrained(config.FINAL_MODEL_DIR)
        base_model_name = peft_config.base_model_name_or_path

        logger.info(f"Base model: {base_model_name}")
        logger.info(f"Loading adapters from: {config.FINAL_MODEL_DIR}")

        base_model = GraphCodeBERTClassifier(base_model_name, config.NUM_LABELS)
        model_with_adapters = PeftModel.from_pretrained(base_model, config.FINAL_MODEL_DIR)
        merged_model = model_with_adapters.merge_and_unload()

        os.makedirs(config.MERGED_MODEL_DIR, exist_ok=True)

        torch.save(merged_model.state_dict(), os.path.join(config.MERGED_MODEL_DIR, "pytorch_model.bin"))
        merged_model.roberta.config.save_pretrained(config.MERGED_MODEL_DIR)

        tokenizer = AutoTokenizer.from_pretrained(base_model_name)
        tokenizer.save_pretrained(config.MERGED_MODEL_DIR)

        model_config = {
            "architectures": ["GraphCodeBERTClassifier"],
            "model_type": "roberta",
            "num_labels": config.NUM_LABELS,
            "base_model": base_model_name,
        }
        with open(os.path.join(config.MERGED_MODEL_DIR, "config.json"), "w") as f:
            json.dump(model_config, f, indent=2)

        logger.info(f"âœ“ Merged model saved: {config.MERGED_MODEL_DIR}")
        logger.info(f"âœ“ Tokenizer saved: {config.MERGED_MODEL_DIR}")

        required_files = ["pytorch_model.bin", "config.json", "tokenizer.json", "tokenizer_config.json"]
        missing_files = [f for f in required_files if not os.path.exists(os.path.join(config.MERGED_MODEL_DIR, f))]

        if missing_files:
            logger.warning(f"âš ï¸ Missing files: {missing_files}")
            return False

        logger.info("âœ“ All required files present")
        return True

    except Exception as e:
        logger.error(f"âŒ Merge failed: {e}")
        import traceback
        traceback.print_exc()
        return False

def upload_to_huggingface(config: Config, logger: logging.Logger, metadata: Dict) -> bool:
    """Upload merged model to HuggingFace Hub using CLI"""
    logger.info("\n" + "=" * 70)
    logger.info("UPLOADING TO HUGGINGFACE HUB")
    logger.info("=" * 70)

    try:
        hf_token = os.getenv("HF_TOKEN")
        if not hf_token:
            logger.warning("âš ï¸ HF_TOKEN not found - skipping upload")
            return False

        readme_content = f"""---
language: code
tags:
- code
- vulnerability-detection
- graphcodebert
- lora
license: mit
datasets:
- custom
metrics:
- f1
- accuracy
---

# CodeGuardian - GraphCodeBERT LoRA v3.1

Fine-tuned GraphCodeBERT model for vulnerability detection using LoRA adapters.

## Model Details

- **Base Model**: {config.MODEL_NAME}
- **Task**: Binary Classification (Secure vs Vulnerable)
- **LoRA Config**: r={config.LORA_R}, Î±={config.LORA_ALPHA}, dropout={config.LORA_DROPOUT}
- **Target Modules**: {config.LORA_TARGET_MODULES}

## Performance

- **Test F1 Score**: {metadata['results']['test_f1']:.4f}
- **Test Accuracy**: {metadata['results']['test_accuracy']:.4f}
- **Test Precision**: {metadata['results']['test_precision']:.4f}
- **Test Recall**: {metadata['results']['test_recall']:.4f}

## Training Details

- **Version**: {metadata['version']}
- **Epochs**: {config.EPOCHS}
- **Batch Size**: {config.TRAIN_BATCH_SIZE} (effective: {config.TRAIN_BATCH_SIZE * config.GRADIENT_ACCUMULATION_STEPS})
- **Learning Rate**: {config.LEARNING_RATE}
- **Weight Decay**: {config.WEIGHT_DECAY}
- **Precision**: {metadata['precision']}
- **Random Seed**: {SEED}
- **Run Hash**: {metadata['run_hash']}

## Usage

```python
from transformers import AutoTokenizer, AutoModel
import torch

tokenizer = AutoTokenizer.from_pretrained("{config.HF_REPO_ID}")
model = AutoModel.from_pretrained("{config.HF_REPO_ID}")

# Your code here
```

## Citation

```bibtex
@software{{codeguardian_graphcodebert,
  author = {{Urva Gandhi}},
  title = {{CodeGuardian - GraphCodeBERT Vulnerability Detection}},
  year = {{2025}},
  url = {{https://huggingface.co/{config.HF_REPO_ID}}}
}}
```
"""

        readme_path = os.path.join(config.MERGED_MODEL_DIR, "README.md")
        with open(readme_path, "w") as f:
            f.write(readme_content)

        logger.info("âœ“ Created model card (README.md)")
        logger.info(f"ðŸš€ Uploading to {config.HF_REPO_ID}...")

        upload_cmd = [
            "huggingface-cli", "upload",
            config.HF_REPO_ID,
            config.MERGED_MODEL_DIR,
            "--repo-type", "model",
            "--commit-message", f"Upload GraphCodeBERT-LoRA v3.1 (F1: {metadata['results']['test_f1']:.4f})"
        ]

        result = subprocess.run(upload_cmd, capture_output=True, text=True)

        if result.returncode == 0:
            logger.info("âœ… Model successfully uploaded to HuggingFace Hub!")
            logger.info(f"ðŸ”— https://huggingface.co/{config.HF_REPO_ID}")
            return True
        else:
            logger.error(f"âŒ Upload failed: {result.stderr}")
            return False

    except FileNotFoundError:
        logger.error("âŒ huggingface-cli not found. Install with: pip install huggingface_hub[cli]")
        return False
    except Exception as e:
        logger.error(f"âŒ Upload failed: {e}")
        import traceback
        traceback.print_exc()
        return False

# ============================================================================
# MAIN TRAINING LOOP
# ============================================================================

def train(config: Config, logger: logging.Logger):
    """Main training function with enhanced stability"""
    start_time = time.time()

    # Copy dataset to local SSD if on Kaggle
    if os.path.exists("/kaggle/input") and not os.path.exists("/kaggle/temp_data"):
        logger.info("=" * 70)
        logger.info("âš¡ COPYING DATASET TO LOCAL SSD (ONE-TIME SETUP)")
        logger.info("=" * 70)
        logger.info("Source: /kaggle/input (NFS - slow)")
        logger.info("Target: /kaggle/temp_data (local SSD - fast)")
        os.system("mkdir -p /kaggle/temp_data && cp -r /kaggle/input/codeguardian-dataset-for-model-fine-tuning /kaggle/temp_data/")
        logger.info("âœ“ Dataset copied to local SSD")
        logger.info("=" * 70)

    run_hash = generate_run_hash(config)
    run_id = f"graphcodebert_lora_{datetime.now().strftime('%Y%m%d_%H%M%S')}_{run_hash}"

    logger.info("=" * 70)
    logger.info("GRAPHCODEBERT LORA FINE-TUNING v3.1")
    logger.info("=" * 70)
    logger.info(f"Run ID: {run_id}")
    logger.info(f"Reproducibility Hash: {run_hash}")
    logger.info(f"Random Seed: {SEED}")
    logger.info(f"Device: {config.DEVICE}")
    logger.info(f"Precision: {'BFloat16' if config.PRECISION_DTYPE == torch.bfloat16 else 'Float16'}")
    logger.info(f"Batch size: {config.TRAIN_BATCH_SIZE} (effective: {config.TRAIN_BATCH_SIZE * config.GRADIENT_ACCUMULATION_STEPS})")
    logger.info(f"Learning rate: {config.LEARNING_RATE}")
    logger.info(f"Epochs: {config.EPOCHS}")
    logger.info(f"LoRA: r={config.LORA_R}, Î±={config.LORA_ALPHA} (Enhanced)")

    if torch.cuda.is_available():
        gpu_name = torch.cuda.get_device_name(0)
        gpu_mem = torch.cuda.get_device_properties(0).total_memory / 1e9
        logger.info(f"\nðŸŽ® GPU: {gpu_name} ({gpu_mem:.1f} GB)")
        capability = torch.cuda.get_device_capability()
        logger.info(f"  Compute Capability: {capability[0]}.{capability[1]}")
        logger.info(f"  BFloat16 Support: {'Yes' if BF16_SUPPORTED else 'No (using FP16)'}")

    for dir_path in [config.CHECKPOINT_DIR, config.METRICS_DIR, config.LOG_DIR]:
        os.makedirs(dir_path, exist_ok=True)

    # Load data
    train_loader, val_loader, test_loader = create_dataloaders(config, logger)

    # Compute class weights
    logger.info("\nComputing class weights...")
    train_data = torch.load(os.path.join(config.DATA_PATH, "train_tokenized_graphcodebert-base.pt"))
    class_weights = compute_class_weights(train_data["labels"], config.DEVICE)
    logger.info(f"Class weights: {class_weights.tolist()}")

    # Initialize model
    model = initialize_model(config, logger)

    # Try torch.compile with OOM safety
    is_compiled = False
    try:
        model = torch.compile(model, mode="reduce-overhead", fullgraph=False)
        is_compiled = True
        logger.info("âœ“ torch.compile enabled (expect ~5-15% speedup after warmup)")
    except RuntimeError as e:
        if "out of memory" in str(e).lower():
            logger.warning("âš ï¸ torch.compile OOM - using eager mode")
            torch.cuda.empty_cache()
        else:
            logger.warning(f"torch.compile disabled: {e}")

    # Setup loss
    if config.USE_FOCAL_LOSS:
        criterion = FocalLoss(gamma=config.FOCAL_GAMMA, alpha=class_weights)
        logger.info(f"Loss: Focal Loss (Î³={config.FOCAL_GAMMA})")
    elif config.USE_WEIGHTED_LOSS:
        criterion = nn.CrossEntropyLoss(weight=class_weights)
        logger.info("Loss: Weighted Cross Entropy")
    else:
        criterion = nn.CrossEntropyLoss()
        logger.info("Loss: Cross Entropy")

    # Setup optimizer with fused mode
    optimizer = torch.optim.AdamW(
        model.parameters(),
        lr=config.LEARNING_RATE,
        weight_decay=config.WEIGHT_DECAY,
        fused=True if torch.cuda.is_available() else False
    )

    # Minimum warmup steps
    num_training_steps = len(train_loader) * config.EPOCHS // config.GRADIENT_ACCUMULATION_STEPS
    warmup_steps = max(50, int(num_training_steps * config.WARMUP_RATIO))
    scheduler = get_linear_schedule_with_warmup(optimizer, warmup_steps, num_training_steps)

    scaler = amp.GradScaler("cuda") if config.USE_MIXED_PRECISION else None

    # Try to resume from checkpoint
    start_epoch, best_f1, was_compiled = load_checkpoint(config.CHECKPOINT_DIR, model, optimizer, scheduler, config, logger)

    logger.info(f"\nTraining steps: {num_training_steps}")
    logger.info(f"Warmup steps: {warmup_steps}")

    # Training loop
    patience = 0
    history = {"train": [], "val": [], "test": None}

    logger.info("\n" + "=" * 70)
    logger.info("STARTING TRAINING")
    logger.info("=" * 70)

    for epoch in range(start_epoch + 1, config.EPOCHS + 1):
        epoch_start = time.time()

        train_metrics = train_epoch(model, train_loader, optimizer, scheduler, scaler, criterion, config, logger, epoch)
        history["train"].append(train_metrics)

        logger.info(f"\nTrain - Loss: {train_metrics['loss']:.4f}, F1: {train_metrics['f1']:.4f}, Acc: {train_metrics['accuracy']:.4f}")

        val_metrics = evaluate(model, val_loader, criterion, config, logger, "val")
        history["val"].append(val_metrics)

        logger.info(f"Val - Loss: {val_metrics['loss']:.4f}, F1: {val_metrics['f1']:.4f}, Acc: {val_metrics['accuracy']:.4f}")
        logger.info(f"Epoch time: {time.time() - epoch_start:.1f}s")

        if (val_metrics["f1"] - best_f1) > config.EARLY_STOPPING_MIN_DELTA:
            best_f1 = val_metrics["f1"]
            patience = 0

            save_checkpoint(epoch, model, optimizer, scheduler, best_f1, config.CHECKPOINT_DIR, logger, is_compiled)

            # Extract base model if compiled
            save_model = model._orig_mod if is_compiled and hasattr(model, "_orig_mod") else model
            save_model.save_pretrained(config.FINAL_MODEL_DIR)
            logger.info(f"âœ“ Best model saved (F1: {best_f1:.4f})")
        else:
            patience += 1
            logger.info(f"Patience: {patience}/{config.EARLY_STOPPING_PATIENCE}")

        if patience >= config.EARLY_STOPPING_PATIENCE:
            logger.info("\nEarly stopping triggered")
            break

    # Load best model for testing
    logger.info("\n" + "=" * 70)
    logger.info("LOADING BEST MODEL")
    logger.info("=" * 70)
    
    # Safe extraction for compiled models
    base_model = model._orig_mod if is_compiled and hasattr(model, "_orig_mod") else model
    if hasattr(base_model, "base_model") and hasattr(base_model.base_model, "model"):
        base_model = base_model.base_model.model
    
    model = PeftModel.from_pretrained(base_model, config.FINAL_MODEL_DIR).to(config.DEVICE)

    # Test evaluation
    logger.info("\n" + "=" * 70)
    logger.info("TEST EVALUATION")
    logger.info("=" * 70)
    test_metrics = evaluate(model, test_loader, criterion, config, logger, "test")
    preds, labels = test_metrics["predictions"], test_metrics["labels"]
    history["test"] = {k: v for k, v in test_metrics.items() if k not in ["predictions", "labels"]}

    logger.info(f"\nTest - Loss: {test_metrics['loss']:.4f}")
    logger.info(f"Test - Accuracy: {test_metrics['accuracy']:.4f}")
    logger.info(f"Test - F1: {test_metrics['f1']:.4f}")
    logger.info(f"Test - Precision: {test_metrics['precision']:.4f}")
    logger.info(f"Test - Recall: {test_metrics['recall']:.4f}")

    cm = confusion_matrix(labels, preds)
    logger.info("\nConfusion Matrix:")
    logger.info(f"  TN={cm[0][0]}, FP={cm[0][1]}")
    logger.info(f"  FN={cm[1][0]}, TP={cm[1][1]}")

    report = classification_report(labels, preds, target_names=["Secure", "Vulnerable"])
    logger.info("\n" + report)

    # Language-wise F1 analysis
    try:
        test_data = torch.load(os.path.join(config.DATA_PATH, "test_tokenized_graphcodebert-base.pt"), map_location="cpu")
        if "language" in test_data:
            import pandas as pd
            languages = test_data["language"].tolist()
            df = pd.DataFrame({"lang": languages, "pred": preds, "label": labels})
            per_lang = []
            for lang, group in df.groupby("lang"):
                f1_lang = f1_score(group["label"], group["pred"], average="binary", zero_division=0)
                acc = accuracy_score(group["label"], group["pred"])
                per_lang.append({"language": lang, "f1": f1_lang, "accuracy": acc, "count": len(group)})
            logger.info("\nðŸ“Š Language-wise F1 Scores:")
            for row in per_lang:
                logger.info(f"  {row['language']:<10s} â†’ F1={row['f1']:.4f}, Acc={row['accuracy']:.4f}, N={row['count']}")
            with open(os.path.join(config.METRICS_DIR, "language_wise_f1.json"), "w") as f:
                json.dump(per_lang, f, indent=2)
            logger.info("âœ“ Saved language-wise metrics")
    except Exception as e:
        logger.warning(f"Language-wise F1 logging skipped: {e}")

    # Save metrics
    metrics_path = os.path.join(config.METRICS_DIR, "results.json")
    with open(metrics_path, "w") as f:
        json.dump(history, f, indent=2, default=str)
    logger.info(f"âœ“ Metrics: {metrics_path}")

    # Save run metadata with version
    metadata = {
        "version": "v3.1 â€“ Stability & Speed Optimized",
        "run_id": run_id,
        "run_hash": run_hash,
        "seed": SEED,
        "timestamp": datetime.now().isoformat(),
        "model": config.MODEL_NAME,
        "lora_config": {
            "r": config.LORA_R,
            "alpha": config.LORA_ALPHA,
            "dropout": config.LORA_DROPOUT,
            "target_modules": config.LORA_TARGET_MODULES
        },
        "training": {
            "epochs": config.EPOCHS,
            "batch_size": config.TRAIN_BATCH_SIZE,
            "learning_rate": config.LEARNING_RATE,
            "weight_decay": config.WEIGHT_DECAY,
            "gradient_accumulation": config.GRADIENT_ACCUMULATION_STEPS
        },
        "results": {
            "best_val_f1": float(best_f1),
            "test_f1": float(test_metrics['f1']),
            "test_accuracy": float(test_metrics['accuracy']),
            "test_precision": float(test_metrics['precision']),
            "test_recall": float(test_metrics['recall'])
        },
        "device": str(config.DEVICE),
        "precision": "BFloat16" if config.PRECISION_DTYPE == torch.bfloat16 else "Float16"
    }

    metadata_path = os.path.join(config.METRICS_DIR, "run_metadata.json")
    with open(metadata_path, "w") as f:
        json.dump(metadata, f, indent=2)
    logger.info(f"âœ“ Run metadata: {metadata_path}")

    merge_success = merge_lora_model(config, logger)

    if merge_success:
        upload_to_huggingface(config, logger, metadata)
    else:
        logger.warning("âš ï¸ Skipping HuggingFace upload due to merge failure")

    # Make results visible in Kaggle Outputs
    os.system("cp -r /kaggle/working/metrics_graphcodebert /kaggle/working/metrics_output_graphcodebert || true")
    os.system("cp -r /kaggle/working/checkpoints_graphcodebert /kaggle/working/checkpoints_output_graphcodebert || true")
    os.system("cp -r /kaggle/working/graphcodebert_lora_merged /kaggle/working/graphcodebert_lora_merged_output || true")
    logger.info("âœ“ Copied outputs to visible Kaggle directories")

    # Explicit cleanup
    del train_loader, val_loader, test_loader, criterion, optimizer, scheduler, scaler, model
    torch.cuda.empty_cache()
    gc.collect()

    total_time = time.time() - start_time
    logger.info("\n" + "=" * 70)
    logger.info("TRAINING COMPLETE")
    logger.info("=" * 70)
    logger.info(f"âœ“ Best Val F1: {best_f1:.4f}")
    logger.info(f"âœ“ Test F1: {test_metrics['f1']:.4f}")
    logger.info(f"âœ“ Total runtime: {total_time/60:.2f} mins")

# ============================================================================
# ENTRY POINT
# ============================================================================

def main():
    parser = argparse.ArgumentParser(description="Fine-tune GraphCodeBERT with LoRA v3.1")
    parser.add_argument("--epochs", type=int, default=None, help="Number of epochs")
    parser.add_argument("--batch_size", type=int, default=None, help="Training batch size")
    parser.add_argument("--lr", type=float, default=None, help="Learning rate")
    args = parser.parse_args()

    config = Config()

    if args.epochs is not None:
        config.EPOCHS = args.epochs
    if args.batch_size is not None:
        config.TRAIN_BATCH_SIZE = args.batch_size
    if args.lr is not None:
        config.LEARNING_RATE = args.lr

    logger = setup_logging(config.LOG_DIR)

    try:
        train(config, logger)
    except Exception as e:
        logger.error(f"Training failed: {e}")
        import traceback
        traceback.print_exc()
        raise

if __name__ == "__main__":
    main()
