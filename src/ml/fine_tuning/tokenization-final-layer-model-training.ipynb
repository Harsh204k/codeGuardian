{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13392594,"sourceType":"datasetVersion","datasetId":8484618}],"dockerImageVersionId":31153,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Step 1: Move to the Kaggle working directory\n%cd /kaggle/working\n\n# Step 2: Pull latest code if repo exists, otherwise clone fresh\n!git -C codeGuardian pull || (rm -rf codeGuardian && git clone https://github.com/Harsh204k/codeGuardian.git)\n\n# Step 3: Move inside the repo\n%cd /kaggle/working/codeGuardian","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install -q -r requirements.txt\n!pip install transformers torch tqdm -q","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-17T04:57:45.993189Z","iopub.execute_input":"2025-10-17T04:57:45.993583Z","iopub.status.idle":"2025-10-17T04:59:33.159726Z","shell.execute_reply.started":"2025-10-17T04:57:45.993548Z","shell.execute_reply":"2025-10-17T04:59:33.158830Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m79.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m494.8/494.8 kB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m564.3/564.3 kB\u001b[0m \u001b[31m32.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.6/61.6 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.8/133.8 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m82.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.4/58.4 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m444.8/444.8 kB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m49.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.5/49.5 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncuml-cu12 25.2.1 requires pylibraft-cu12==25.2.*, which is not installed.\ncuml-cu12 25.2.1 requires rmm-cu12==25.2.*, which is not installed.\ncudf-cu12 25.2.2 requires rmm-cu12==25.2.*, which is not installed.\ncuvs-cu12 25.2.1 requires pylibraft-cu12==25.2.*, which is not installed.\nbigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\ncategory-encoders 2.7.0 requires scikit-learn<1.6.0,>=1.0.0, but you have scikit-learn 1.7.2 which is incompatible.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\nsklearn-compat 0.1.3 requires scikit-learn<1.7,>=1.2, but you have scikit-learn 1.7.2 which is incompatible.\nbigframes 2.12.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\nthinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":17},{"cell_type":"markdown","source":"# ***Tokenization***","metadata":{}},{"cell_type":"markdown","source":"> ***codeBERT Tokenization***","metadata":{}},{"cell_type":"code","source":"# Step 1: Always start from the Kaggle working directory\n%cd /kaggle/working/codeGuardian\n\n# Step 2: Change into the scripts/validation folder (if it exists)\nimport os\ntarget_path = \"/kaggle/working/codeGuardian/src/ml/tokenization\"\n\nif os.path.exists(target_path):\n    %cd $target_path\n    print(\"✅ Successfully moved to:\", target_path)\n    !ls\nelse:\n    print(\"⚠️ Folder not found:\", target_path)\n    print(\"Available directories:\")\n    !ls /kaggle/working/codeGuardian","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-15T00:35:10.668066Z","iopub.execute_input":"2025-10-15T00:35:10.668557Z","iopub.status.idle":"2025-10-15T00:35:10.803896Z","shell.execute_reply.started":"2025-10-15T00:35:10.668512Z","shell.execute_reply":"2025-10-15T00:35:10.802567Z"}},"outputs":[{"name":"stdout","text":"/kaggle/working/codeGuardian\n/kaggle/working/codeGuardian/src/ml/tokenization\n✅ Successfully moved to: /kaggle/working/codeGuardian/src/ml/tokenization\n__init__.py  run_both_tokenizations.py\ttokenize_graphcodebert.py\nREADME.md    tokenize_codebert.py\ttokenizer_config.yml\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import os\n\n# Define the full path to your script\nscript_path = \"/kaggle/working/codeGuardian/src/ml/tokenization/tokenize_codebert.py\"\n\n# Check if the script exists before running\nif os.path.exists(script_path):\n    print(f\"✅ Found script at: {script_path}\")\n    \n    # Run the script\n    exit_code = os.system(\"python tokenize_codebert.py\")\n    \n    # Check if it ran successfully\n    if exit_code == 0:\n        print(\"🎉 Script executed successfully!\")\n    else:\n        print(f\"⚠️ Script exited with code: {exit_code}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-15T00:35:10.806101Z","iopub.execute_input":"2025-10-15T00:35:10.806520Z","iopub.status.idle":"2025-10-15T00:51:52.904805Z","shell.execute_reply.started":"2025-10-15T00:35:10.806485Z","shell.execute_reply":"2025-10-15T00:51:52.903324Z"}},"outputs":[{"name":"stdout","text":"✅ Found script at: /kaggle/working/codeGuardian/src/ml/tokenization/tokenize_codebert.py\n","output_type":"stream"},{"name":"stderr","text":"Extracting data: 100%|██████████| 507487/507487 [00:00<00:00, 602717.85it/s]\nTokenizing train: 100%|██████████| 51/51 [07:13<00:00,  8.51s/it]\nExtracting data: 100%|██████████| 63436/63436 [00:00<00:00, 708892.04it/s]\nTokenizing val: 100%|██████████| 7/7 [00:49<00:00,  7.11s/it]\nExtracting data: 100%|██████████| 63436/63436 [00:00<00:00, 473982.97it/s]\nTokenizing test: 100%|██████████| 7/7 [00:48<00:00,  6.95s/it]\n","output_type":"stream"},{"name":"stdout","text":"\n✅ GOOD SCORE! Pipeline completed with some warnings. ✅\n🎉 Script executed successfully!\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"> ***graphCodeBERT Tokenization***","metadata":{}},{"cell_type":"code","source":"# Step 1: Always start from the Kaggle working directory\n%cd /kaggle/working/codeGuardian\n\n# Step 2: Change into the scripts/validation folder (if it exists)\nimport os\ntarget_path = \"/kaggle/working/codeGuardian/src/ml/tokenization\"\n\nif os.path.exists(target_path):\n    %cd $target_path\n    print(\"✅ Successfully moved to:\", target_path)\n    !ls\nelse:\n    print(\"⚠️ Folder not found:\", target_path)\n    print(\"Available directories:\")\n    !ls /kaggle/working/codeGuardian","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-17T11:43:02.860351Z","iopub.execute_input":"2025-10-17T11:43:02.860623Z","iopub.status.idle":"2025-10-17T11:43:02.985387Z","shell.execute_reply.started":"2025-10-17T11:43:02.860600Z","shell.execute_reply":"2025-10-17T11:43:02.984631Z"}},"outputs":[{"name":"stdout","text":"/kaggle/working/codeGuardian\n/kaggle/working/codeGuardian/src/ml/tokenization\n✅ Successfully moved to: /kaggle/working/codeGuardian/src/ml/tokenization\n__init__.py  run_both_tokenizations.py\ttokenize_graphcodebert.py\nREADME.md    tokenize_codebert.py\ttokenizer_config.yml\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import os\n\n# Define the full path to your script\nscript_path = \"/kaggle/working/codeGuardian/src/ml/tokenization/tokenize_graphcodebert.py\"\n\n# Check if the script exists before running\nif os.path.exists(script_path):\n    print(f\"✅ Found script at: {script_path}\")\n    \n    # Run the script\n    exit_code = os.system(\"python tokenize_graphcodebert.py\")\n    \n    # Check if it ran successfully\n    if exit_code == 0:\n        print(\"🎉 Script executed successfully!\")\n    else:\n        print(f\"⚠️ Script exited with code: {exit_code}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-15T00:51:53.076251Z","iopub.execute_input":"2025-10-15T00:51:53.076688Z","iopub.status.idle":"2025-10-15T01:03:00.229445Z","shell.execute_reply.started":"2025-10-15T00:51:53.076644Z","shell.execute_reply":"2025-10-15T01:03:00.227783Z"}},"outputs":[{"name":"stdout","text":"✅ Found script at: /kaggle/working/codeGuardian/src/ml/tokenization/tokenize_graphcodebert.py\n","output_type":"stream"},{"name":"stderr","text":"Extracting data: 100%|██████████| 507487/507487 [00:00<00:00, 567588.42it/s]\nTokenizing train: 100%|██████████| 51/51 [07:39<00:00,  9.01s/it]\nExtracting data: 100%|██████████| 63436/63436 [00:00<00:00, 691300.65it/s]\nTokenizing val: 100%|██████████| 7/7 [00:50<00:00,  7.28s/it]\nExtracting data: 100%|██████████| 63436/63436 [00:00<00:00, 405076.23it/s]\nTokenizing test: 100%|██████████| 7/7 [00:49<00:00,  7.05s/it]\n","output_type":"stream"},{"name":"stdout","text":"\n✅ GOOD SCORE! Pipeline completed with some warnings. ✅\n🎉 Script executed successfully!\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"# ***Final Layer Fine-Tuning***","metadata":{}},{"cell_type":"code","source":"!pip uninstall -y sentence-transformers datasets featuretools umap-learn libcugraph-cu12 pydantic pylibraft-cu12 rmm-cu12\n\nprint(\"Installing dependencies...\")\n!pip install -q \\\n    transformers==4.36.0 \\\n    peft==0.7.1 \\\n    scikit-learn \\\n    tqdm \\\n    torch==2.1.0 \\\n    torchvision \\\n    torchaudio \\\n    datasets \\\n    \"rmm-cu12==25.6.*\" \\\n    \"pylibraft-cu12==25.6.*\" \\\n    \"libraft-cu12==25.6.*\" \\\n    \"pydantic>=2.0,<2.12\"\n\nprint(\"✓ Dependencies installed\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-17T18:24:07.272101Z","iopub.execute_input":"2025-10-17T18:24:07.272713Z","iopub.status.idle":"2025-10-17T18:24:14.739130Z","shell.execute_reply.started":"2025-10-17T18:24:07.272687Z","shell.execute_reply":"2025-10-17T18:24:14.738369Z"}},"outputs":[{"name":"stdout","text":"\u001b[33mWARNING: Skipping sentence-transformers as it is not installed.\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Skipping datasets as it is not installed.\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Skipping featuretools as it is not installed.\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Skipping umap-learn as it is not installed.\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Skipping libcugraph-cu12 as it is not installed.\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Skipping pydantic as it is not installed.\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Skipping pylibraft-cu12 as it is not installed.\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Skipping rmm-cu12 as it is not installed.\u001b[0m\u001b[33m\n\u001b[0mInstalling dependencies...\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.8/126.8 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.6/68.6 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h\u001b[31mERROR: Cannot install libraft-cu12==25.6.0 and torch==2.1.0 because these package versions have conflicting dependencies.\u001b[0m\u001b[31m\n\u001b[0m\u001b[31mERROR: ResolutionImpossible: for help visit https://pip.pypa.io/en/latest/topics/dependency-resolution/#dealing-with-dependency-conflicts\u001b[0m\u001b[31m\n\u001b[0m✓ Dependencies installed\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"import os\n\nprint(\"\\nVerifying dataset...\")\n\n# Check CodeBERT dataset\ncodebert_path = \"/kaggle/input/codeguardian-dataset-for-model-fine-tuning/tokenized/codebert\"\nif os.path.exists(codebert_path):\n    print(f\"✓ CodeBERT dataset found: {codebert_path}\")\n    print(f\"  Files: {os.listdir(codebert_path)}\")\nelse:\n    print(f\"❌ CodeBERT dataset not found: {codebert_path}\")\n\n# Check GraphCodeBERT dataset\ngraphcodebert_path = \"/kaggle/input/codeguardian-dataset-for-model-fine-tuning/tokenized/graphcodebert\"\nif os.path.exists(graphcodebert_path):\n    print(f\"✓ GraphCodeBERT dataset found: {graphcodebert_path}\")\n    print(f\"  Files: {os.listdir(graphcodebert_path)}\")\nelse:\n    print(f\"❌ GraphCodeBERT dataset not found: {graphcodebert_path}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-17T18:24:32.456296Z","iopub.execute_input":"2025-10-17T18:24:32.456584Z","iopub.status.idle":"2025-10-17T18:24:32.477099Z","shell.execute_reply.started":"2025-10-17T18:24:32.456560Z","shell.execute_reply":"2025-10-17T18:24:32.476282Z"}},"outputs":[{"name":"stdout","text":"\nVerifying dataset...\n✓ CodeBERT dataset found: /kaggle/input/codeguardian-dataset-for-model-fine-tuning/tokenized/codebert\n  Files: ['val_tokenized_codebert.pt', 'test_tokenized_codebert.pt', 'train_tokenized_codebert.pt']\n✓ GraphCodeBERT dataset found: /kaggle/input/codeguardian-dataset-for-model-fine-tuning/tokenized/graphcodebert\n  Files: ['test_tokenized_graphcodebert.pt', 'val_tokenized_graphcodebert.pt', 'train_tokenized_graphcodebert.pt']\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"import torch\nprint(\"Checking CodeBERT train.pt...\")\ndata = torch.load('/kaggle/input/codeguardian-dataset-for-model-fine-tuning/tokenized/codebert/train_tokenized_codebert.pt')\nprint(f\"Keys: {data.keys()}\")\nprint(f\"Input IDs shape: {data['input_ids'].shape}\")\nprint(f\"Labels shape: {data['labels'].shape}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-17T18:24:35.641130Z","iopub.execute_input":"2025-10-17T18:24:35.641650Z","iopub.status.idle":"2025-10-17T18:24:40.320293Z","shell.execute_reply.started":"2025-10-17T18:24:35.641627Z","shell.execute_reply":"2025-10-17T18:24:40.319501Z"}},"outputs":[{"name":"stdout","text":"Checking CodeBERT train.pt...\nKeys: dict_keys(['input_ids', 'attention_mask', 'labels'])\nInput IDs shape: torch.Size([507487, 512])\nLabels shape: torch.Size([507487])\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"# Step 1: Always start from the Kaggle working directory\n%cd /kaggle/working/codeGuardian\n\n# Step 2: Change into the scripts/validation folder (if it exists)\nimport os\ntarget_path = \"/kaggle/working/codeGuardian/src/ml/fine_tuning\"\n\nif os.path.exists(target_path):\n    %cd $target_path\n    print(\"✅ Successfully moved to:\", target_path)\n    !ls\nelse:\n    print(\"⚠️ Folder not found:\", target_path)\n    print(\"Available directories:\")\n    !ls /kaggle/working/codeGuardian","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-17T18:24:43.727475Z","iopub.execute_input":"2025-10-17T18:24:43.728215Z","iopub.status.idle":"2025-10-17T18:24:43.967427Z","shell.execute_reply.started":"2025-10-17T18:24:43.728188Z","shell.execute_reply":"2025-10-17T18:24:43.966475Z"}},"outputs":[{"name":"stdout","text":"/kaggle/working/codeGuardian\n/kaggle/working/codeGuardian/src/ml/fine_tuning\n✅ Successfully moved to: /kaggle/working/codeGuardian/src/ml/fine_tuning\ncodebert_detector.py  graphcodebert_detector.py  train_codebert_lora.py\nfine_tune_config.yml  __init__.py\t\t train_graphcodebert_lora.py\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"import os\n\n# Define the full path to your script\nscript_path = \"/kaggle/working/codeGuardian/src/ml/fine_tuning/train_codebert_lora.py\"\n\n# Check if the script exists before running\nif os.path.exists(script_path):\n    print(f\"✅ Found script at: {script_path}\")\n    \n    # Run the script\n    print(\"\\n\" + \"=\"*70)\n    print(\"STARTING CODEBERT TRAINING\")\n    print(\"=\"*70 + \"\\n\")\n    \n    exit_code = os.system(\"python train_codebert_lora.py\")\n    \n    # Check if it ran successfully\n    if exit_code == 0:\n        print(\"🎉 Script executed successfully!\")\n    else:\n        print(f\"⚠️ Script exited with code: {exit_code}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-17T18:24:48.487230Z","iopub.execute_input":"2025-10-17T18:24:48.487834Z","iopub.status.idle":"2025-10-17T22:38:35.388412Z","shell.execute_reply.started":"2025-10-17T18:24:48.487803Z","shell.execute_reply":"2025-10-17T22:38:35.387726Z"}},"outputs":[{"name":"stdout","text":"✅ Found script at: /kaggle/working/codeGuardian/src/ml/fine_tuning/train_codebert_lora.py\n\n======================================================================\nSTARTING CODEBERT TRAINING\n======================================================================\n\n","output_type":"stream"},{"name":"stderr","text":"2025-10-17 18:24:56.922296: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1760725496.944325     268 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1760725496.951193     268 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"🔧 Checking dependencies...\n✓ Dependencies updated successfully\n\n======================================================================\nCODEGUARDIAN - CODEBERT FINE-TUNING WITH LORA (OPTIMIZED)\n======================================================================\nDevice: cuda\nPrecision: Float16 (Mixed Precision)\nTrain Batch Size: 64\nEval Batch Size: 128\nGradient Accumulation: 2x\nEffective Batch Size: 128\nLearning Rate: 0.002\nEpochs: 3\n\n🎮 GPU Info:\n  - Name: Tesla T4\n  - Memory: 15.83 GB\n  - Compute Capability: 7.5\n  - BFloat16 Support: No (using FP16)\n\n======================================================================\nLOADING DATASETS\n======================================================================\nLoading dataset from: /kaggle/input/codeguardian-dataset-for-model-fine-tuning/tokenized/codebert/train_tokenized_codebert.pt\n✓ Loaded 507487 samples\n  - Input shape: torch.Size([507487, 512])\n  - Labels distribution: tensor([404520, 102967])\nLoading dataset from: /kaggle/input/codeguardian-dataset-for-model-fine-tuning/tokenized/codebert/val_tokenized_codebert.pt\n✓ Loaded 63436 samples\n  - Input shape: torch.Size([63436, 512])\n  - Labels distribution: tensor([50565, 12871])\nLoading dataset from: /kaggle/input/codeguardian-dataset-for-model-fine-tuning/tokenized/codebert/test_tokenized_codebert.pt\n✓ Loaded 63436 samples\n  - Input shape: torch.Size([63436, 512])\n  - Labels distribution: tensor([50565, 12871])\n\n✓ DataLoaders created successfully\n  - Train batches: 7930\n  - Val batches: 496\n  - Test batches: 496\n\n======================================================================\nINITIALIZING MODEL\n======================================================================\nLoading model: microsoft/codebert-base\n✓ Loaded successfully using AutoModel\n✓ Base model loaded: microsoft/codebert-base\n\n📊 Parameters before LoRA:\n  - Total: 124,647,170\n  - Trainable: 1,538 (0.00%)\n\n✓ LoRA applied to final classification layer\n  - LoRA rank (r): 8\n  - LoRA alpha: 16\n  - LoRA dropout: 0.1\n\n📊 Parameters after LoRA:\n  - Trainable: 32,258 (0.03%)\n  - Memory reduction: 99.97%\n\n⚠️ torch.compile() disabled for PEFT compatibility\n   Training will still be optimized with mixed precision & gradient accumulation\n\n======================================================================\nSTARTING TRAINING\n======================================================================\n\n======================================================================\nEPOCH 1/3\n======================================================================\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/3 [TRAIN]: 100%|████████████| 7930/7930 [1:13:25<00:00,  1.80it/s, loss=0.3155, lr=1.35e-03]\n","output_type":"stream"},{"name":"stdout","text":"\n📊 Train Metrics:\n  - Loss: 0.3155\n  - Accuracy: 0.8771\n  - F1-Score: 0.6356\n  - Precision: 0.7976\n  - Recall: 0.5284\n","output_type":"stream"},{"name":"stderr","text":"VALIDATION: 100%|█████████████████████████████████████████████████| 496/496 [08:09<00:00,  1.01it/s]\n","output_type":"stream"},{"name":"stdout","text":"\n📊 Validation Metrics:\n  - Loss: 0.2197\n  - Accuracy: 0.9204\n  - F1-Score: 0.7768\n  - Precision: 0.9009\n  - Recall: 0.6827\n\n✓ Best model saved! (F1: 0.7768)\n✓ Epoch 1 checkpoint saved: /kaggle/working/fine-tuning/codebert_lora_epoch_1.pt\n\n======================================================================\nEPOCH 2/3\n======================================================================\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/3 [TRAIN]: 100%|████████████| 7930/7930 [1:13:35<00:00,  1.80it/s, loss=0.2855, lr=6.75e-04]\n","output_type":"stream"},{"name":"stdout","text":"\n📊 Train Metrics:\n  - Loss: 0.2854\n  - Accuracy: 0.8903\n  - F1-Score: 0.6838\n  - Precision: 0.8236\n  - Recall: 0.5846\n","output_type":"stream"},{"name":"stderr","text":"VALIDATION: 100%|█████████████████████████████████████████████████| 496/496 [08:09<00:00,  1.01it/s]\n","output_type":"stream"},{"name":"stdout","text":"\n📊 Validation Metrics:\n  - Loss: 0.2179\n  - Accuracy: 0.9214\n  - F1-Score: 0.7773\n  - Precision: 0.9149\n  - Recall: 0.6757\n\n✓ Best model saved! (F1: 0.7773)\n✓ Epoch 2 checkpoint saved: /kaggle/working/fine-tuning/codebert_lora_epoch_2.pt\n\n======================================================================\nEPOCH 3/3\n======================================================================\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/3 [TRAIN]: 100%|████████████| 7930/7930 [1:13:34<00:00,  1.80it/s, loss=0.2669, lr=2.54e-06]\n","output_type":"stream"},{"name":"stdout","text":"\n📊 Train Metrics:\n  - Loss: 0.2669\n  - Accuracy: 0.8978\n  - F1-Score: 0.7107\n  - Precision: 0.8348\n  - Recall: 0.6186\n","output_type":"stream"},{"name":"stderr","text":"VALIDATION: 100%|█████████████████████████████████████████████████| 496/496 [08:09<00:00,  1.01it/s]\n","output_type":"stream"},{"name":"stdout","text":"\n📊 Validation Metrics:\n  - Loss: 0.2032\n  - Accuracy: 0.9281\n  - F1-Score: 0.8028\n  - Precision: 0.9045\n  - Recall: 0.7217\n\n✓ Best model saved! (F1: 0.8028)\n✓ Epoch 3 checkpoint saved: /kaggle/working/fine-tuning/codebert_lora_epoch_3.pt\n\n======================================================================\nLOADING BEST MODEL FOR FINAL EVALUATION\n======================================================================\n✓ Loaded model from epoch 3 (F1: 0.8028)\n\n======================================================================\nFINAL TEST EVALUATION\n======================================================================\n","output_type":"stream"},{"name":"stderr","text":"TEST: 100%|███████████████████████████████████████████████████████| 496/496 [08:08<00:00,  1.01it/s]\n","output_type":"stream"},{"name":"stdout","text":"\n📊 Test Metrics:\n  - Loss: 0.2061\n  - Accuracy: 0.9259\n  - F1-Score: 0.7965\n  - Precision: 0.8996\n  - Recall: 0.7146\n\n✓ Metrics saved to: /kaggle/working/fine-tuning/codebert_eval_metrics.json\n\n======================================================================\nCLEANING UP GPU MEMORY\n======================================================================\n✓ GPU memory cleaned\n\n======================================================================\nTRAINING COMPLETE!\n======================================================================\n✓ Best Validation F1: 0.8028\n✓ Test F1: 0.7965\n✓ Model saved: /kaggle/working/fine-tuning/codebert_final_layer.pt\n✓ Metrics saved: /kaggle/working/fine-tuning/codebert_eval_metrics.json\n\n======================================================================\nTRAINING SUMMARY\n======================================================================\n⏱  Total Runtime: 253.51 minutes (15210.9 seconds)\n⏱  Avg Time per Epoch: 73.56 minutes (4413.8 seconds)\n💾 GPU Memory:\n  - Allocated: 1.02 GB\n  - Reserved: 1.06 GB\n\n📊 Final Performance:\n  - Best Val F1: 0.8028\n  - Test Accuracy: 0.9259\n  - Test F1: 0.7965\n  - Test Precision: 0.8996\n  - Test Recall: 0.7146\n🎉 Script executed successfully!\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"import os\n\n# Define the full path to your script\nscript_path = \"/kaggle/working/codeGuardian/src/ml/fine_tuning/train_graphcodebert_lora.py\"\n\n# Check if the script exists before running\nif os.path.exists(script_path):\n    print(f\"✅ Found script at: {script_path}\")\n    \n    # Run the script\n    print(\"\\n\" + \"=\"*70)\n    print(\"STARTING CODEBERT TRAINING\")\n    print(\"=\"*70 + \"\\n\")\n    \n    exit_code = os.system(\"python train_codebert_lora.py\")\n    \n    # Check if it ran successfully\n    if exit_code == 0:\n        print(\"🎉 Script executed successfully!\")\n    else:\n        print(f\"⚠️ Script exited with code: {exit_code}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-17T22:38:35.389680Z","iopub.execute_input":"2025-10-17T22:38:35.389899Z","iopub.status.idle":"2025-10-17T22:47:06.209357Z","shell.execute_reply.started":"2025-10-17T22:38:35.389882Z","shell.execute_reply":"2025-10-17T22:47:06.208509Z"}},"outputs":[{"name":"stdout","text":"✅ Found script at: /kaggle/working/codeGuardian/src/ml/fine_tuning/train_graphcodebert_lora.py\n\n======================================================================\nSTARTING CODEBERT TRAINING\n======================================================================\n\n","output_type":"stream"},{"name":"stderr","text":"2025-10-17 22:38:44.429113: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1760740724.452160     327 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1760740724.459298     327 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"🔧 Checking dependencies...\n✓ Dependencies updated successfully\n\n======================================================================\nCODEGUARDIAN - CODEBERT FINE-TUNING WITH LORA (OPTIMIZED)\n======================================================================\nDevice: cuda\nPrecision: Float16 (Mixed Precision)\nTrain Batch Size: 64\nEval Batch Size: 128\nGradient Accumulation: 2x\nEffective Batch Size: 128\nLearning Rate: 0.002\nEpochs: 3\n\n🎮 GPU Info:\n  - Name: Tesla T4\n  - Memory: 15.83 GB\n  - Compute Capability: 7.5\n  - BFloat16 Support: No (using FP16)\n\n======================================================================\nLOADING DATASETS\n======================================================================\nLoading dataset from: /kaggle/input/codeguardian-dataset-for-model-fine-tuning/tokenized/codebert/train_tokenized_codebert.pt\n✓ Loaded 507487 samples\n  - Input shape: torch.Size([507487, 512])\n  - Labels distribution: tensor([404520, 102967])\nLoading dataset from: /kaggle/input/codeguardian-dataset-for-model-fine-tuning/tokenized/codebert/val_tokenized_codebert.pt\n✓ Loaded 63436 samples\n  - Input shape: torch.Size([63436, 512])\n  - Labels distribution: tensor([50565, 12871])\nLoading dataset from: /kaggle/input/codeguardian-dataset-for-model-fine-tuning/tokenized/codebert/test_tokenized_codebert.pt\n✓ Loaded 63436 samples\n  - Input shape: torch.Size([63436, 512])\n  - Labels distribution: tensor([50565, 12871])\n\n✓ DataLoaders created successfully\n  - Train batches: 7930\n  - Val batches: 496\n  - Test batches: 496\n\n======================================================================\nINITIALIZING MODEL\n======================================================================\nLoading model: microsoft/codebert-base\n✓ Loaded successfully using AutoModel\n✓ Base model loaded: microsoft/codebert-base\n\n📊 Parameters before LoRA:\n  - Total: 124,647,170\n  - Trainable: 1,538 (0.00%)\n\n✓ LoRA applied to final classification layer\n  - LoRA rank (r): 8\n  - LoRA alpha: 16\n  - LoRA dropout: 0.1\n\n📊 Parameters after LoRA:\n  - Trainable: 32,258 (0.03%)\n  - Memory reduction: 99.97%\n\n⚠️ torch.compile() disabled for PEFT compatibility\n   Training will still be optimized with mixed precision & gradient accumulation\n\n======================================================================\nTRAINING ALREADY COMPLETE\n======================================================================\n✓ All 3 epochs already trained\n✓ Loading final model for evaluation\n\n======================================================================\nTRAINING SKIPPED\n======================================================================\n\n======================================================================\nLOADING BEST MODEL FOR FINAL EVALUATION\n======================================================================\n✓ Loaded model from epoch 3 (F1: 0.8028)\n\n======================================================================\nFINAL TEST EVALUATION\n======================================================================\n","output_type":"stream"},{"name":"stderr","text":"TEST: 100%|███████████████████████████████████████████████████████| 496/496 [08:08<00:00,  1.02it/s]\n","output_type":"stream"},{"name":"stdout","text":"\n📊 Test Metrics:\n  - Loss: 0.2061\n  - Accuracy: 0.9259\n  - F1-Score: 0.7965\n  - Precision: 0.8996\n  - Recall: 0.7146\n\n✓ Metrics saved to: /kaggle/working/fine-tuning/codebert_eval_metrics.json\n\n======================================================================\nCLEANING UP GPU MEMORY\n======================================================================\n✓ GPU memory cleaned\n\n======================================================================\nTRAINING COMPLETE!\n======================================================================\n✓ Best Validation F1: 0.8028\n✓ Test F1: 0.7965\n✓ Model saved: /kaggle/working/fine-tuning/codebert_final_layer.pt\n✓ Metrics saved: /kaggle/working/fine-tuning/codebert_eval_metrics.json\n\n======================================================================\nTRAINING SUMMARY\n======================================================================\n⏱  Total Runtime: 8.24 minutes (494.6 seconds)\n⏱  Avg Time per Epoch: 0.00 minutes (0.0 seconds)\n💾 GPU Memory:\n  - Allocated: 1.01 GB\n  - Reserved: 1.15 GB\n\n📊 Final Performance:\n  - Best Val F1: 0.8028\n  - Test Accuracy: 0.9259\n  - Test F1: 0.7965\n  - Test Precision: 0.8996\n  - Test Recall: 0.7146\n🎉 Script executed successfully!\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}