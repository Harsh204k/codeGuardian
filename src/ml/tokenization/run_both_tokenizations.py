#!/usr/bin/env python
"""
CodeGuardian: Run Both Tokenization Pipelines
==============================================
Orchestrates CodeBERT and GraphCodeBERT tokenization for vulnerability detection.

Author: Urva Gandhi
Usage: python run_both_tokenizations.py
"""

import sys
import os
import time
from pathlib import Path

# Add tokenization scripts to path
tokenization_dir = Path(__file__).parent
sys.path.insert(0, str(tokenization_dir))

print("=" * 80)
print("üöÄ CodeGuardian Tokenization Pipeline Runner")
print("=" * 80)
print("Author: Urva Gandhi")
print("Purpose: Tokenize code dataset for LoRA fine-tuning")
print("=" * 80)

start_time = time.time()

# ============================================================================
# 1Ô∏è‚É£ CodeBERT Tokenization
# ============================================================================
print("\n" + "=" * 80)
print("üì¶ STEP 1: CodeBERT Tokenization")
print("=" * 80)

codebert_success = False
try:
    from tokenize_codebert import main as tokenize_codebert_main

    print("Starting CodeBERT tokenization...")
    exit_code = tokenize_codebert_main()

    if exit_code == 0:
        print("\n‚úÖ CodeBERT tokenization completed successfully!")
        codebert_success = True
    else:
        print(f"\n‚ö†Ô∏è CodeBERT tokenization completed with warnings (exit code: {exit_code})")

except Exception as e:
    print(f"\n‚ùå CodeBERT tokenization failed: {e}")
    import traceback
    traceback.print_exc()


# ============================================================================
# 2Ô∏è‚É£ GraphCodeBERT Tokenization
# ============================================================================
print("\n" + "=" * 80)
print("üì¶ STEP 2: GraphCodeBERT Tokenization")
print("=" * 80)

graphcodebert_success = False
try:
    from tokenize_graphcodebert import main as tokenize_graphcodebert_main

    print("Starting GraphCodeBERT tokenization...")
    exit_code = tokenize_graphcodebert_main()

    if exit_code == 0:
        print("\n‚úÖ GraphCodeBERT tokenization completed successfully!")
        graphcodebert_success = True
    else:
        print(f"\n‚ö†Ô∏è GraphCodeBERT tokenization completed with warnings (exit code: {exit_code})")

except Exception as e:
    print(f"\n‚ùå GraphCodeBERT tokenization failed: {e}")
    import traceback
    traceback.print_exc()


# ============================================================================
# 3Ô∏è‚É£ Verify Output Files
# ============================================================================
print("\n" + "=" * 80)
print("ÔøΩ OUTPUT FILE VERIFICATION")
print("=" * 80)

output_base = "/kaggle/working/tokenized"
splits = ["train", "val", "test"]
models = ["codebert", "graphcodebert"]

all_files_exist = True

for model in models:
    model_dir = os.path.join(output_base, model)
    print(f"\n{model.upper()}:")

    if not os.path.exists(model_dir):
        print(f"  ‚ùå Directory not found: {model_dir}")
        all_files_exist = False
        continue

    for split in splits:
        file_path = os.path.join(model_dir, f"{split}_tokenized.pt")
        if os.path.exists(file_path):
            size_mb = os.path.getsize(file_path) / (1024 * 1024)
            print(f"  ‚úì {split}_tokenized.pt ({size_mb:.2f} MB)")
        else:
            print(f"  ‚ùå {split}_tokenized.pt - NOT FOUND")
            all_files_exist = False


# ============================================================================
# 4Ô∏è‚É£ Summary
# ============================================================================
elapsed_time = time.time() - start_time

print("\n" + "=" * 80)
print("üìä FINAL SUMMARY")
print("=" * 80)

print(f"\n‚è±Ô∏è  Total Runtime: {elapsed_time/60:.2f} minutes")

print(f"\nüìà Pipeline Status:")
print(f"  CodeBERT:       {'‚úÖ SUCCESS' if codebert_success else '‚ùå FAILED'}")
print(f"  GraphCodeBERT:  {'‚úÖ SUCCESS' if graphcodebert_success else '‚ùå FAILED'}")
print(f"  All Files:      {'‚úÖ PRESENT' if all_files_exist else '‚ùå MISSING'}")

if codebert_success and graphcodebert_success and all_files_exist:
    print("\nüéâ TOKENIZATION COMPLETED SUCCESSFULLY!")
    print("\n‚úÖ All tokenized files are ready for training")
    print("\nüí° Next Steps:")
    print("  1. Run validation notebook: validate_tokenization.ipynb")
    print("  2. Start LoRA fine-tuning: train_codebert_lora.py")
    print("  3. Train GraphCodeBERT: train_graphcodebert_lora.py")
    print("  4. Create hybrid ensemble model")
    exit_code = 0
elif codebert_success or graphcodebert_success:
    print("\n‚ö†Ô∏è PARTIAL SUCCESS")
    print("\nSome tokenization pipelines completed successfully.")
    print("Review the logs above for details on failures.")
    exit_code = 1
else:
    print("\n‚ùå TOKENIZATION FAILED")
    print("\nBoth pipelines encountered errors.")
    print("Please review the error messages above and:")
    print("  1. Check input JSONL files exist")
    print("  2. Verify sufficient disk space")
    print("  3. Ensure dependencies are installed")
    print("  4. Check error logs for details")
    exit_code = 2

print("\nüìö Documentation:")
print("  - Tokenization README: src/ml/tokenization/README.md")
print("  - Validation Notebook: src/ml/tokenization/validate_tokenization.ipynb")

print("=" * 80)

sys.exit(exit_code)
