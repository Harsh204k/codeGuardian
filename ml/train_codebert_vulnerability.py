#!/usr/bin/env python3
"""
Enhanced ML Training Pipeline for CodeGuardian
Trains CodeBERT model on DiverseVul dataset for vulnerability detection
"""

import json
import torch
import numpy as np
import pandas as pd
from pathlib import Path
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix
from transformers import (
    AutoTokenizer, AutoModelForSequenceClassification,
    TrainingArguments, Trainer,
    EarlyStoppingCallback
)
from datasets import Dataset
import logging

# Set up logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class DiverseVulDataProcessor:
    """Process DiverseVul dataset for training"""
    
    def __init__(self, dataset_path="DiverseVul Dataset/diversevul_20230702.json"):
        self.dataset_path = Path(dataset_path)
        self.data = []
        
    def load_dataset(self, max_samples=None):
        """Load and parse DiverseVul JSON lines format"""
        logger.info(f"Loading dataset from {self.dataset_path}")
        
        with open(self.dataset_path, 'r', encoding='utf-8') as f:
            for i, line in enumerate(f):
                if max_samples and i >= max_samples:
                    break
                    
                line = line.strip()
                if line:
                    try:
                        item = json.loads(line)
                        # Filter out very short or very long code samples
                        code = item.get('func', '')
                        if 50 <= len(code) <= 8000:  # Reasonable code length
                            self.data.append({
                                'code': code,
                                'label': int(item.get('target', 0)),
                                'cwe': item.get('cwe', []),
                                'project': item.get('project', ''),
                                'size': item.get('size', 0)
                            })
                    except json.JSONDecodeError:
                        continue
                        
                if i % 10000 == 0:
                    logger.info(f"Processed {i} samples")
        
        logger.info(f"Loaded {len(self.data)} samples")
        return self.data
    
    def create_balanced_dataset(self, max_per_class=15000):
        """Create balanced dataset for training"""
        vulnerable = [item for item in self.data if item['label'] == 1]
        safe = [item for item in self.data if item['label'] == 0]
        
        logger.info(f"Original: {len(vulnerable)} vulnerable, {len(safe)} safe")
        
        # Sample balanced amounts
        vulnerable_sample = vulnerable[:min(max_per_class, len(vulnerable))]
        safe_sample = safe[:min(max_per_class, len(safe))]
        
        balanced_data = vulnerable_sample + safe_sample
        np.random.shuffle(balanced_data)
        
        logger.info(f"Balanced: {len(vulnerable_sample)} vulnerable, {len(safe_sample)} safe")
        return balanced_data

class VulnerabilityTrainer:
    """Train CodeBERT for vulnerability detection"""
    
    def __init__(self, model_name="microsoft/codebert-base"):
        self.model_name = model_name
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.model = AutoModelForSequenceClassification.from_pretrained(
            model_name, 
            num_labels=2,
            problem_type="single_label_classification"
        )
        
        # Add padding token if not present
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
            self.model.config.pad_token_id = self.tokenizer.eos_token_id
    
    def tokenize_function(self, examples):
        """Tokenize code samples"""
        return self.tokenizer(
            examples['code'],
            truncation=True,
            padding=True,
            max_length=512,
            return_tensors="pt"
        )
    
    def prepare_datasets(self, data):
        """Prepare training and validation datasets"""
        # Split data
        train_data, val_data = train_test_split(
            data, test_size=0.2, random_state=42,
            stratify=[item['label'] for item in data]
        )
        
        # Convert to DataFrames
        train_df = pd.DataFrame(train_data)
        val_df = pd.DataFrame(val_data)
        
        # Create Hugging Face datasets
        train_dataset = Dataset.from_pandas(train_df)
        val_dataset = Dataset.from_pandas(val_df)
        
        # Tokenize
        train_dataset = train_dataset.map(self.tokenize_function, batched=True)
        val_dataset = val_dataset.map(self.tokenize_function, batched=True)
        
        # Set format for PyTorch
        train_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])
        val_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])
        
        return train_dataset, val_dataset
    
    def compute_metrics(self, eval_pred):
        """Compute metrics for evaluation"""
        predictions, labels = eval_pred
        predictions = np.argmax(predictions, axis=1)
        
        precision, recall, f1, _ = precision_recall_fscore_support(
            labels, predictions, average='binary'
        )
        accuracy = accuracy_score(labels, predictions)
        
        return {
            'accuracy': accuracy,
            'f1': f1,
            'precision': precision,
            'recall': recall
        }
    
    def train(self, train_dataset, val_dataset, output_dir="models/codebert_vulnerability"):
        """Train the model"""
        training_args = TrainingArguments(
            output_dir=output_dir,
            num_train_epochs=3,
            per_device_train_batch_size=8,
            per_device_eval_batch_size=8,
            warmup_steps=500,
            weight_decay=0.01,
            logging_dir=f'{output_dir}/logs',
            logging_steps=100,
            evaluation_strategy="steps",
            eval_steps=500,
            save_strategy="steps",
            save_steps=1000,
            load_best_model_at_end=True,
            metric_for_best_model="f1",
            greater_is_better=True,
            report_to=None,  # Disable wandb
            dataloader_num_workers=0,  # Avoid multiprocessing issues
        )
        
        trainer = Trainer(
            model=self.model,
            args=training_args,
            train_dataset=train_dataset,
            eval_dataset=val_dataset,
            compute_metrics=self.compute_metrics,
            callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]
        )
        
        # Train the model
        logger.info("Starting training...")
        trainer.train()
        
        # Save the model
        trainer.save_model()
        self.tokenizer.save_pretrained(output_dir)
        
        logger.info(f"Model saved to {output_dir}")
        return trainer

def main():
    """Main training pipeline"""
    # Set random seeds for reproducibility
    torch.manual_seed(42)
    np.random.seed(42)
    
    # Check if CUDA is available
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    logger.info(f"Using device: {device}")
    
    # Load and process data
    processor = DiverseVulDataProcessor()
    data = processor.load_dataset(max_samples=50000)  # Start with subset for speed
    balanced_data = processor.create_balanced_dataset(max_per_class=10000)
    
    # Initialize trainer
    trainer = VulnerabilityTrainer()
    
    # Prepare datasets
    train_dataset, val_dataset = trainer.prepare_datasets(balanced_data)
    
    logger.info(f"Training dataset size: {len(train_dataset)}")
    logger.info(f"Validation dataset size: {len(val_dataset)}")
    
    # Train model
    trained_trainer = trainer.train(train_dataset, val_dataset)
    
    # Evaluate on validation set
    logger.info("Evaluating model...")
    results = trained_trainer.evaluate()
    
    print("\nðŸŽ¯ TRAINING RESULTS:")
    print(f"Accuracy: {results['eval_accuracy']:.3f}")
    print(f"F1-Score: {results['eval_f1']:.3f}")
    print(f"Precision: {results['eval_precision']:.3f}")
    print(f"Recall: {results['eval_recall']:.3f}")
    
    return trained_trainer

if __name__ == "__main__":
    main()